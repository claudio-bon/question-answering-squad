# -*- coding: utf-8 -*-

import collections


Position = collections.namedtuple("Position", ["start","end"])


def tokenize(tokenizer, max_length, stride, row):
    """
    Apply the Tokenizer's tokenization's function.
    
    Parameters
    ----------
    tokenizer : transformers.PreTrainedTokenizer
        Pretrained Tokenizer.
    max_length : int
        The maximum length of a feature.
    stride : int
        Overlap between two context when truncating.
    row : dict
        Row to tokenize

    Returns
    -------
    transformers.BatchEncoding
        Tokenized rows.
        
    """
    # The input data must be tokenized in the order dictated by the side the
    # selected checkpoint apply the padding
    pad_on_right = tokenizer.padding_side == "right"
    
    return tokenizer(
        row["question" if pad_on_right else "context"],
        row["context" if pad_on_right else "question"],
        max_length=max_length,
        truncation="only_second" if pad_on_right else "only_first",
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        stride=stride,
        padding="max_length",
    )


def get_context_position_in_tokenized_input(tokenized_row, i, pad_on_right):
    """
    Since the tokenizer returns a sequence composed of the concatenated and
    tokenized question and input this function retrieve the starting and ending
    context's tokens inside the said sequence.

    Parameters
    ----------
    tokenized_row : transformers.BatchEncoding
        Tokenized dataset.
    i : int
        Row index.
    pad_on_right : boolean
        True if the model's tokenizer pad on the right.

    Returns
    -------
    collections.namedtuple
        Start and end token indices of the context inside the tokenized input.

    """
    # List that holds for each index (up to the lenght of the tokenized input sequence)
    # 1 if its corresponding token is a context's token, 0 if it's a question's token
    # (the contrair if pad_on_right is true). Null for the special tokens.
    sequence_ids = tokenized_row.sequence_ids(i)

    # Start context's token's index inside the input sequence.
    token_start_index = sequence_ids.index(1 if pad_on_right else 0)

    # End context's token's index inside the input sequence.
    token_end_index = len(sequence_ids) - list(reversed(sequence_ids)).index(1 if pad_on_right else 0)

    return Position(start=token_start_index, end=token_end_index)


def preprocess_eval(tokenizer, max_length, stride):
    """
    Incapsulate the implementation of the preprocessing function

    Parameters
    ----------
    tokenizer : transformers.PreTrainedTokenizer
        Pretrained Tokenizer.
    max_length : int
        The maximum length of a feature.
    stride : int
        Overlap between two context when truncating.

    Returns
    -------
    function
        Implementation of the preprocessing function.

    """
    pad_on_right = tokenizer.padding_side == "right"
    def preprocess_eval_impl(rows):
        """
        Preprocess the dataset and prepare it for the prediction phase.
        Its main task is the tokenization of the input.
        In addition it also adds to the features two informations:
            - Which context a features belong to (remember that since a token
              is truncated and splitted in more sequences this is not trivial
              anymore)
            - The start and end context's token's indices inside the tokenized
              sequence (which is composed of the question plus the context)

        Parameters
        ----------
        rows : dict
            Input that has to be preprocessed.

        Returns
        -------
        tokenized_rows : transformers.BatchEncoding
            Preprocessed rows.

        """
        # Tokenize the rows
        tokenized_rows = tokenize(tokenizer, max_length, stride, rows)

        # overflow_to_sample_mapping keeps the corrispondence between a feature and the row it was generated by.
        sample_mapping = tokenized_rows.pop("overflow_to_sample_mapping")

        # For each feature save the row that generated it.
        tokenized_rows["row_id"] = [rows["id"][sample_index] for sample_index in sample_mapping]

        # Save the start and end context's token's position inside the tokenized input sequence (composed by question plus context)
        context_pos = [get_context_position_in_tokenized_input(tokenized_rows,i,pad_on_right) for i in range(len(tokenized_rows["input_ids"]))]
        tokenized_rows["context_start"], tokenized_rows["context_end"] = [index.start for index in context_pos], [index.end for index in context_pos]

        return tokenized_rows
    return preprocess_eval_impl